Types of Normalisation: Batch Norm, Layer Norm, Instance Norm, Group Norm
Dropout only in training time not test time
Many layers of sigmoids lead to smaller and smaller gradients
GELU: nice behaviour around 0, smoothness facilitates training in practice
KAiming Initialisation sqrt(2/dim)
Transfer learning can be understood as a feature extractor where the frozen part extracts and then the FC layer classifies
Random Search better than Grid Search